{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe62198a-5d5d-4a3a-b572-97a5a5218d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9217537a-6c5a-4c11-b52f-ae39955e12d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.config.run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03663ffa-9118-477f-af88-331ed884b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2a81373-97ce-4d64-b7f4-1b8fb52ed228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7f4b198d6c70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e510f23e-4056-4963-b13c-eb415d0e4718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04325246, 0.0483912 , 0.03049769, 0.0138689 ], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cc61418-7b01-4426-937e-b3f07bc21896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eba87445-7671-49d9-93d0-7f6d4555e053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=600x400 at 0x7FE31C04B070>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIL.Image.fromarray(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "679ea28b-dbd7-4701-8bae-8b64b7626e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0. , 6. , 0.7, 0.8]]), (1, 4))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_ip1 = np.asarray([0.0,6.0,0.7,0.8])\n",
    "state_ip1 = state_ip1[np.newaxis,:]\n",
    "state_ip1,state_ip1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff986b7c-47a0-4ef3-87a0-b824700c56a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.zeros([1,2])\n",
    "y_true[np.arange(1),1] = 1.0\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "278500a5-a8ef-4706-b15b-bc82ee418b7f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delat:KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='input_50'), name='input_50', description=\"created by layer 'input_50'\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_87/3826455724.py\", line 12, in custom_policy_loss  *\n        d = tf.keras.backend.get_value(delta)\n    File \"/mnt/c/Users/hockg/Documents/tensorflow_env/lib/python3.8/site-packages/keras/backend.py\", line 3838, in get_value  **\n        return x.numpy()\n\n    AttributeError: 'KerasTensor' object has no attribute 'numpy'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_87/772591907.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate_ip1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/c/Users/hockg/Documents/tensorflow_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/hockg/Documents/tensorflow_env/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    697\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_87/3826455724.py\", line 12, in custom_policy_loss  *\n        d = tf.keras.backend.get_value(delta)\n    File \"/mnt/c/Users/hockg/Documents/tensorflow_env/lib/python3.8/site-packages/keras/backend.py\", line 3838, in get_value  **\n        return x.numpy()\n\n    AttributeError: 'KerasTensor' object has no attribute 'numpy'\n"
     ]
    }
   ],
   "source": [
    "actor.fit(x=[state_ip1,np.expand_dims(12,0)],y=y_true,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78fdc732-9449-4d69-84da-295dec4abda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "from actor_critic_tf import actor_critic as ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb46dd9e-5dad-419c-8601-ca8166d191ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "act = ac.ActorCritic(0.001,0.99,0.001,4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae04a9bf-828b-48fe-872d-b42898c5f41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 1s - loss: 99.9748 - 585ms/epoch - 585ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f81ebe514f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act.critic_net.fit(x=state_ip1,y=np.expand_dims(12,0),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b518ec24-86ed-4016-a9f6-0780bacd16d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=7.5>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(state_ip1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12bdbd74-741f-44db-b694-b30a6a6eb86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 1s - loss: 7.6217 - 718ms/epoch - 718ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f044866ee50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act.actor_net.fit(x=[state_ip1,np.expand_dims(12,0)],y=y_true,verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2983b263-050d-47fb-90e3-2d5511bba06f",
   "metadata": {},
   "source": [
    "## Gym env params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7471cd2f-8310-4577-92da-1e4aa66a593d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_shape = env.observation_space.shape[0]\n",
    "state_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0285b7e-f6e1-4248-aeaf-376fab5be1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_shape = env.action_space.n\n",
    "action_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9bb849-af9e-4bfd-930f-7365a165b0bf",
   "metadata": {},
   "source": [
    "## Actor critic import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3c1dcc7-baa3-4c8b-87a5-d93ef07168c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from actor_critic_tf import actor_critic as ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da85abf5-5161-4684-9279-a199d2f3f0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 00:08:44.013423: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: UNKNOWN ERROR (100)\n",
      "2022-01-21 00:08:44.013476: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (MSI): /proc/driver/nvidia/version does not exist\n",
      "2022-01-21 00:08:44.013696: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/mnt/c/Users/hockg/Documents/tensorflow_env/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "act = ac.ActorCritic(alpha=0.001,beta=0.001,gamma=0.99,state_dim=state_shape,action_dim=action_shape,num_of_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b915b02f-514c-4caa-925e-6ad9c599ee76",
   "metadata": {},
   "source": [
    "## Build policy and critic network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32964fa5-22bf-4500-8fc2-a7a87116118f",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6406bc4a-9e0e-4610-b40f-f1a1d919cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episode = 100\n",
    "num_steps = 200\n",
    "gamma = 0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16386013-9c87-484d-9508-59e43830016e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "198dcb81-45cf-4128-92d5-c31c162fa04e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:0 , Avg reward: 1.0909090909090908, Cumulative rewards: 12.0\n",
      "episode:1 , Avg reward: 1.04, Cumulative rewards: 26.0\n",
      "episode:2 , Avg reward: 1.1111111111111112, Cumulative rewards: 10.0\n",
      "episode:3 , Avg reward: 1.0714285714285714, Cumulative rewards: 15.0\n",
      "episode:4 , Avg reward: 1.0833333333333333, Cumulative rewards: 13.0\n",
      "episode:5 , Avg reward: 1.0555555555555556, Cumulative rewards: 19.0\n",
      "episode:6 , Avg reward: 1.0769230769230769, Cumulative rewards: 14.0\n",
      "episode:7 , Avg reward: 1.1111111111111112, Cumulative rewards: 10.0\n",
      "episode:8 , Avg reward: 1.0833333333333333, Cumulative rewards: 13.0\n",
      "episode:9 , Avg reward: 1.0909090909090908, Cumulative rewards: 12.0\n",
      "episode:10 , Avg reward: 1.0666666666666667, Cumulative rewards: 16.0\n",
      "episode:11 , Avg reward: 1.0833333333333333, Cumulative rewards: 13.0\n",
      "episode:12 , Avg reward: 1.125, Cumulative rewards: 9.0\n",
      "episode:13 , Avg reward: 1.1, Cumulative rewards: 11.0\n",
      "episode:14 , Avg reward: 1.1111111111111112, Cumulative rewards: 10.0\n",
      "episode:15 , Avg reward: 1.0714285714285714, Cumulative rewards: 15.0\n",
      "episode:16 , Avg reward: 1.0666666666666667, Cumulative rewards: 16.0\n",
      "episode:17 , Avg reward: 1.0625, Cumulative rewards: 17.0\n",
      "episode:18 , Avg reward: 1.1111111111111112, Cumulative rewards: 10.0\n",
      "episode:19 , Avg reward: 1.0909090909090908, Cumulative rewards: 12.0\n",
      "episode:20 , Avg reward: 1.125, Cumulative rewards: 9.0\n",
      "episode:21 , Avg reward: 1.1, Cumulative rewards: 11.0\n",
      "episode:22 , Avg reward: 1.0714285714285714, Cumulative rewards: 15.0\n",
      "episode:23 , Avg reward: 1.0833333333333333, Cumulative rewards: 13.0\n",
      "episode:24 , Avg reward: 1.0625, Cumulative rewards: 17.0\n",
      "episode:25 , Avg reward: 1.1428571428571428, Cumulative rewards: 8.0\n",
      "episode:26 , Avg reward: 1.1, Cumulative rewards: 11.0\n",
      "episode:27 , Avg reward: 1.1, Cumulative rewards: 11.0\n",
      "episode:28 , Avg reward: 1.0769230769230769, Cumulative rewards: 14.0\n",
      "episode:29 , Avg reward: 1.1111111111111112, Cumulative rewards: 10.0\n",
      "episode:30 , Avg reward: 1.0476190476190477, Cumulative rewards: 22.0\n",
      "episode:31 , Avg reward: 1.0222222222222221, Cumulative rewards: 46.0\n",
      "episode:32 , Avg reward: 1.0476190476190477, Cumulative rewards: 22.0\n",
      "episode:33 , Avg reward: 1.0833333333333333, Cumulative rewards: 13.0\n",
      "episode:34 , Avg reward: 1.0192307692307692, Cumulative rewards: 53.0\n",
      "episode:35 , Avg reward: 1.0909090909090908, Cumulative rewards: 12.0\n",
      "episode:36 , Avg reward: 1.0625, Cumulative rewards: 17.0\n",
      "episode:37 , Avg reward: 1.0526315789473684, Cumulative rewards: 20.0\n",
      "episode:38 , Avg reward: 1.05, Cumulative rewards: 21.0\n",
      "episode:39 , Avg reward: 1.0909090909090908, Cumulative rewards: 12.0\n",
      "episode:40 , Avg reward: 1.0188679245283019, Cumulative rewards: 54.0\n",
      "episode:41 , Avg reward: 1.0454545454545454, Cumulative rewards: 23.0\n",
      "episode:42 , Avg reward: 1.0555555555555556, Cumulative rewards: 19.0\n",
      "episode:43 , Avg reward: 1.0384615384615385, Cumulative rewards: 27.0\n",
      "episode:44 , Avg reward: 1.0434782608695652, Cumulative rewards: 24.0\n",
      "episode:45 , Avg reward: 1.0769230769230769, Cumulative rewards: 14.0\n",
      "episode:46 , Avg reward: 1.0434782608695652, Cumulative rewards: 24.0\n",
      "episode:47 , Avg reward: 1.0833333333333333, Cumulative rewards: 13.0\n",
      "episode:48 , Avg reward: 1.0526315789473684, Cumulative rewards: 20.0\n",
      "episode:49 , Avg reward: 1.0625, Cumulative rewards: 17.0\n",
      "episode:50 , Avg reward: 1.024390243902439, Cumulative rewards: 42.0\n",
      "episode:51 , Avg reward: 1.0151515151515151, Cumulative rewards: 67.0\n",
      "episode:52 , Avg reward: 1.0769230769230769, Cumulative rewards: 14.0\n",
      "episode:53 , Avg reward: 1.0769230769230769, Cumulative rewards: 14.0\n",
      "episode:54 , Avg reward: 1.0454545454545454, Cumulative rewards: 23.0\n",
      "episode:55 , Avg reward: 1.032258064516129, Cumulative rewards: 32.0\n",
      "episode:56 , Avg reward: 1.025, Cumulative rewards: 41.0\n",
      "episode:57 , Avg reward: 1.0256410256410255, Cumulative rewards: 40.0\n",
      "episode:58 , Avg reward: 1.015625, Cumulative rewards: 65.0\n",
      "episode:59 , Avg reward: 1.0285714285714285, Cumulative rewards: 36.0\n",
      "episode:60 , Avg reward: 1.0714285714285714, Cumulative rewards: 15.0\n",
      "episode:61 , Avg reward: 1.0212765957446808, Cumulative rewards: 48.0\n",
      "episode:62 , Avg reward: 1.0294117647058822, Cumulative rewards: 35.0\n",
      "episode:63 , Avg reward: 1.0285714285714285, Cumulative rewards: 36.0\n",
      "episode:64 , Avg reward: 1.0096153846153846, Cumulative rewards: 105.0\n",
      "episode:65 , Avg reward: 1.0294117647058822, Cumulative rewards: 35.0\n",
      "episode:66 , Avg reward: 1.0222222222222221, Cumulative rewards: 46.0\n",
      "episode:67 , Avg reward: 1.0294117647058822, Cumulative rewards: 35.0\n",
      "episode:68 , Avg reward: 1.0126582278481013, Cumulative rewards: 80.0\n",
      "episode:69 , Avg reward: 1.0138888888888888, Cumulative rewards: 73.0\n",
      "episode:70 , Avg reward: 1.013157894736842, Cumulative rewards: 77.0\n",
      "episode:71 , Avg reward: 1.0084745762711864, Cumulative rewards: 119.0\n",
      "episode:72 , Avg reward: 1.0277777777777777, Cumulative rewards: 37.0\n",
      "episode:73 , Avg reward: 1.0112359550561798, Cumulative rewards: 90.0\n",
      "episode:74 , Avg reward: 1.04, Cumulative rewards: 26.0\n",
      "episode:75 , Avg reward: 1.0138888888888888, Cumulative rewards: 73.0\n",
      "episode:76 , Avg reward: 1.0232558139534884, Cumulative rewards: 44.0\n",
      "episode:77 , Avg reward: 1.0172413793103448, Cumulative rewards: 59.0\n",
      "episode:78 , Avg reward: 1.0666666666666667, Cumulative rewards: 16.0\n",
      "episode:79 , Avg reward: 1.0169491525423728, Cumulative rewards: 60.0\n",
      "episode:80 , Avg reward: 1.0769230769230769, Cumulative rewards: 14.0\n",
      "episode:81 , Avg reward: 1.0666666666666667, Cumulative rewards: 16.0\n",
      "episode:82 , Avg reward: 1.0625, Cumulative rewards: 17.0\n",
      "episode:83 , Avg reward: 1.0625, Cumulative rewards: 17.0\n",
      "episode:84 , Avg reward: 1.032258064516129, Cumulative rewards: 32.0\n",
      "episode:85 , Avg reward: 1.0666666666666667, Cumulative rewards: 16.0\n",
      "episode:86 , Avg reward: 1.027027027027027, Cumulative rewards: 38.0\n",
      "episode:87 , Avg reward: 1.04, Cumulative rewards: 26.0\n",
      "episode:88 , Avg reward: 1.0166666666666666, Cumulative rewards: 61.0\n",
      "episode:89 , Avg reward: 1.0277777777777777, Cumulative rewards: 37.0\n",
      "episode:90 , Avg reward: 1.0108695652173914, Cumulative rewards: 93.0\n",
      "episode:91 , Avg reward: 1.027027027027027, Cumulative rewards: 38.0\n",
      "episode:92 , Avg reward: 1.025, Cumulative rewards: 41.0\n",
      "episode:93 , Avg reward: 1.0256410256410255, Cumulative rewards: 40.0\n",
      "episode:94 , Avg reward: 1.032258064516129, Cumulative rewards: 32.0\n",
      "episode:95 , Avg reward: 1.0416666666666667, Cumulative rewards: 25.0\n",
      "episode:96 , Avg reward: 1.03125, Cumulative rewards: 33.0\n",
      "episode:97 , Avg reward: 1.04, Cumulative rewards: 26.0\n",
      "episode:98 , Avg reward: 1.0084033613445378, Cumulative rewards: 120.0\n",
      "episode:99 , Avg reward: 1.0113636363636365, Cumulative rewards: 89.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_episode):\n",
    "    done = False\n",
    "    Rewards = 0\n",
    "    state = env.reset()\n",
    "    current_value = 0.0\n",
    "    for j in range(num_steps):\n",
    "        action = act.choose_choose_action(state)\n",
    "        next_state,reward,done,info = env.step(action)\n",
    "        Rewards += reward \n",
    "        act.learn(state,action,reward,next_state,done)\n",
    "        if done or j == 199:\n",
    "            print(f'episode:{i} , Avg reward: {Rewards/j}, Cumulative rewards: {Rewards}')\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
