{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe62198a-5d5d-4a3a-b572-97a5a5218d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9217537a-6c5a-4c11-b52f-ae39955e12d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.config.run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03663ffa-9118-477f-af88-331ed884b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2a81373-97ce-4d64-b7f4-1b8fb52ed228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7f4b198d6c70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e510f23e-4056-4963-b13c-eb415d0e4718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04325246, 0.0483912 , 0.03049769, 0.0138689 ], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cc61418-7b01-4426-937e-b3f07bc21896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eba87445-7671-49d9-93d0-7f6d4555e053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=600x400 at 0x7FE31C04B070>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIL.Image.fromarray(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2983b263-050d-47fb-90e3-2d5511bba06f",
   "metadata": {},
   "source": [
    "## Gym env params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7471cd2f-8310-4577-92da-1e4aa66a593d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_shape = env.observation_space.shape[0]\n",
    "state_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0285b7e-f6e1-4248-aeaf-376fab5be1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_shape = env.action_space.n\n",
    "action_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9bb849-af9e-4bfd-930f-7365a165b0bf",
   "metadata": {},
   "source": [
    "## Actor critic import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3c1dcc7-baa3-4c8b-87a5-d93ef07168c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from actor_critic_tf import actor_critic as ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da85abf5-5161-4684-9279-a199d2f3f0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 00:08:44.013423: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: UNKNOWN ERROR (100)\n",
      "2022-01-21 00:08:44.013476: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (MSI): /proc/driver/nvidia/version does not exist\n",
      "2022-01-21 00:08:44.013696: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/mnt/c/Users/hockg/Documents/tensorflow_env/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "act = ac.ActorCritic(alpha=0.001,beta=0.001,gamma=0.99,state_dim=state_shape,action_dim=action_shape,num_of_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b915b02f-514c-4caa-925e-6ad9c599ee76",
   "metadata": {},
   "source": [
    "## Build policy and critic network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32964fa5-22bf-4500-8fc2-a7a87116118f",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6406bc4a-9e0e-4610-b40f-f1a1d919cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episode = 100\n",
    "num_steps = 200\n",
    "gamma = 0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16386013-9c87-484d-9508-59e43830016e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "198dcb81-45cf-4128-92d5-c31c162fa04e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:0 , Avg reward: 1.0909090909090908, Cumulative rewards: 12.0\n",
      "episode:1 , Avg reward: 1.04, Cumulative rewards: 26.0\n",
      "episode:2 , Avg reward: 1.1111111111111112, Cumulative rewards: 10.0\n",
      "episode:3 , Avg reward: 1.0714285714285714, Cumulative rewards: 15.0\n",
      "episode:4 , Avg reward: 1.0833333333333333, Cumulative rewards: 13.0\n",
      "episode:5 , Avg reward: 1.0555555555555556, Cumulative rewards: 19.0\n",
      "episode:6 , Avg reward: 1.0769230769230769, Cumulative rewards: 14.0\n",
      "episode:7 , Avg reward: 1.1111111111111112, Cumulative rewards: 10.0\n",
      "episode:8 , Avg reward: 1.0833333333333333, Cumulative rewards: 13.0\n",
      "episode:9 , Avg reward: 1.0909090909090908, Cumulative rewards: 12.0\n",
      "episode:10 , Avg reward: 1.0666666666666667, Cumulative rewards: 16.0\n",
      "episode:11 , Avg reward: 1.0833333333333333, Cumulative rewards: 13.0\n",
      "episode:12 , Avg reward: 1.125, Cumulative rewards: 9.0\n",
      "episode:13 , Avg reward: 1.1, Cumulative rewards: 11.0\n",
      "episode:14 , Avg reward: 1.1111111111111112, Cumulative rewards: 10.0\n",
      "episode:15 , Avg reward: 1.0714285714285714, Cumulative rewards: 15.0\n",
      "episode:16 , Avg reward: 1.0666666666666667, Cumulative rewards: 16.0\n",
      "episode:17 , Avg reward: 1.0625, Cumulative rewards: 17.0\n",
      "episode:18 , Avg reward: 1.1111111111111112, Cumulative rewards: 10.0\n",
      "episode:19 , Avg reward: 1.0909090909090908, Cumulative rewards: 12.0\n",
      "episode:20 , Avg reward: 1.125, Cumulative rewards: 9.0\n",
      "episode:21 , Avg reward: 1.1, Cumulative rewards: 11.0\n",
      "episode:22 , Avg reward: 1.0714285714285714, Cumulative rewards: 15.0\n",
      "episode:23 , Avg reward: 1.0833333333333333, Cumulative rewards: 13.0\n",
      "episode:24 , Avg reward: 1.0625, Cumulative rewards: 17.0\n",
      "episode:25 , Avg reward: 1.1428571428571428, Cumulative rewards: 8.0\n",
      "episode:26 , Avg reward: 1.1, Cumulative rewards: 11.0\n",
      "episode:27 , Avg reward: 1.1, Cumulative rewards: 11.0\n",
      "episode:28 , Avg reward: 1.0769230769230769, Cumulative rewards: 14.0\n",
      "episode:29 , Avg reward: 1.1111111111111112, Cumulative rewards: 10.0\n",
      "episode:30 , Avg reward: 1.0476190476190477, Cumulative rewards: 22.0\n",
      "episode:31 , Avg reward: 1.0222222222222221, Cumulative rewards: 46.0\n",
      "episode:32 , Avg reward: 1.0476190476190477, Cumulative rewards: 22.0\n",
      "episode:33 , Avg reward: 1.0833333333333333, Cumulative rewards: 13.0\n",
      "episode:34 , Avg reward: 1.0192307692307692, Cumulative rewards: 53.0\n",
      "episode:35 , Avg reward: 1.0909090909090908, Cumulative rewards: 12.0\n",
      "episode:36 , Avg reward: 1.0625, Cumulative rewards: 17.0\n",
      "episode:37 , Avg reward: 1.0526315789473684, Cumulative rewards: 20.0\n",
      "episode:38 , Avg reward: 1.05, Cumulative rewards: 21.0\n",
      "episode:39 , Avg reward: 1.0909090909090908, Cumulative rewards: 12.0\n",
      "episode:40 , Avg reward: 1.0188679245283019, Cumulative rewards: 54.0\n",
      "episode:41 , Avg reward: 1.0454545454545454, Cumulative rewards: 23.0\n",
      "episode:42 , Avg reward: 1.0555555555555556, Cumulative rewards: 19.0\n",
      "episode:43 , Avg reward: 1.0384615384615385, Cumulative rewards: 27.0\n",
      "episode:44 , Avg reward: 1.0434782608695652, Cumulative rewards: 24.0\n",
      "episode:45 , Avg reward: 1.0769230769230769, Cumulative rewards: 14.0\n",
      "episode:46 , Avg reward: 1.0434782608695652, Cumulative rewards: 24.0\n",
      "episode:47 , Avg reward: 1.0833333333333333, Cumulative rewards: 13.0\n",
      "episode:48 , Avg reward: 1.0526315789473684, Cumulative rewards: 20.0\n",
      "episode:49 , Avg reward: 1.0625, Cumulative rewards: 17.0\n",
      "episode:50 , Avg reward: 1.024390243902439, Cumulative rewards: 42.0\n",
      "episode:51 , Avg reward: 1.0151515151515151, Cumulative rewards: 67.0\n",
      "episode:52 , Avg reward: 1.0769230769230769, Cumulative rewards: 14.0\n",
      "episode:53 , Avg reward: 1.0769230769230769, Cumulative rewards: 14.0\n",
      "episode:54 , Avg reward: 1.0454545454545454, Cumulative rewards: 23.0\n",
      "episode:55 , Avg reward: 1.032258064516129, Cumulative rewards: 32.0\n",
      "episode:56 , Avg reward: 1.025, Cumulative rewards: 41.0\n",
      "episode:57 , Avg reward: 1.0256410256410255, Cumulative rewards: 40.0\n",
      "episode:58 , Avg reward: 1.015625, Cumulative rewards: 65.0\n",
      "episode:59 , Avg reward: 1.0285714285714285, Cumulative rewards: 36.0\n",
      "episode:60 , Avg reward: 1.0714285714285714, Cumulative rewards: 15.0\n",
      "episode:61 , Avg reward: 1.0212765957446808, Cumulative rewards: 48.0\n",
      "episode:62 , Avg reward: 1.0294117647058822, Cumulative rewards: 35.0\n",
      "episode:63 , Avg reward: 1.0285714285714285, Cumulative rewards: 36.0\n",
      "episode:64 , Avg reward: 1.0096153846153846, Cumulative rewards: 105.0\n",
      "episode:65 , Avg reward: 1.0294117647058822, Cumulative rewards: 35.0\n",
      "episode:66 , Avg reward: 1.0222222222222221, Cumulative rewards: 46.0\n",
      "episode:67 , Avg reward: 1.0294117647058822, Cumulative rewards: 35.0\n",
      "episode:68 , Avg reward: 1.0126582278481013, Cumulative rewards: 80.0\n",
      "episode:69 , Avg reward: 1.0138888888888888, Cumulative rewards: 73.0\n",
      "episode:70 , Avg reward: 1.013157894736842, Cumulative rewards: 77.0\n",
      "episode:71 , Avg reward: 1.0084745762711864, Cumulative rewards: 119.0\n",
      "episode:72 , Avg reward: 1.0277777777777777, Cumulative rewards: 37.0\n",
      "episode:73 , Avg reward: 1.0112359550561798, Cumulative rewards: 90.0\n",
      "episode:74 , Avg reward: 1.04, Cumulative rewards: 26.0\n",
      "episode:75 , Avg reward: 1.0138888888888888, Cumulative rewards: 73.0\n",
      "episode:76 , Avg reward: 1.0232558139534884, Cumulative rewards: 44.0\n",
      "episode:77 , Avg reward: 1.0172413793103448, Cumulative rewards: 59.0\n",
      "episode:78 , Avg reward: 1.0666666666666667, Cumulative rewards: 16.0\n",
      "episode:79 , Avg reward: 1.0169491525423728, Cumulative rewards: 60.0\n",
      "episode:80 , Avg reward: 1.0769230769230769, Cumulative rewards: 14.0\n",
      "episode:81 , Avg reward: 1.0666666666666667, Cumulative rewards: 16.0\n",
      "episode:82 , Avg reward: 1.0625, Cumulative rewards: 17.0\n",
      "episode:83 , Avg reward: 1.0625, Cumulative rewards: 17.0\n",
      "episode:84 , Avg reward: 1.032258064516129, Cumulative rewards: 32.0\n",
      "episode:85 , Avg reward: 1.0666666666666667, Cumulative rewards: 16.0\n",
      "episode:86 , Avg reward: 1.027027027027027, Cumulative rewards: 38.0\n",
      "episode:87 , Avg reward: 1.04, Cumulative rewards: 26.0\n",
      "episode:88 , Avg reward: 1.0166666666666666, Cumulative rewards: 61.0\n",
      "episode:89 , Avg reward: 1.0277777777777777, Cumulative rewards: 37.0\n",
      "episode:90 , Avg reward: 1.0108695652173914, Cumulative rewards: 93.0\n",
      "episode:91 , Avg reward: 1.027027027027027, Cumulative rewards: 38.0\n",
      "episode:92 , Avg reward: 1.025, Cumulative rewards: 41.0\n",
      "episode:93 , Avg reward: 1.0256410256410255, Cumulative rewards: 40.0\n",
      "episode:94 , Avg reward: 1.032258064516129, Cumulative rewards: 32.0\n",
      "episode:95 , Avg reward: 1.0416666666666667, Cumulative rewards: 25.0\n",
      "episode:96 , Avg reward: 1.03125, Cumulative rewards: 33.0\n",
      "episode:97 , Avg reward: 1.04, Cumulative rewards: 26.0\n",
      "episode:98 , Avg reward: 1.0084033613445378, Cumulative rewards: 120.0\n",
      "episode:99 , Avg reward: 1.0113636363636365, Cumulative rewards: 89.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_episode):\n",
    "    done = False\n",
    "    Rewards = 0\n",
    "    state = env.reset()\n",
    "    current_value = 0.0\n",
    "    for j in range(num_steps):\n",
    "        action = act.choose_choose_action(state)\n",
    "        next_state,reward,done,info = env.step(action)\n",
    "        Rewards += reward \n",
    "        act.learn(state,action,reward,next_state,done)\n",
    "        if done or j == num_steps - 1:\n",
    "            print(f'episode:{i} , Avg reward: {Rewards/j}, Cumulative rewards: {Rewards}')\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
