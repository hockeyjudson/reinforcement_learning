{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f212cba-b0b6-487f-aa7d-fb1103853f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d521265-33d4-47ae-a0ee-c7a6abe72d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\").unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73c529d3-a869-4517-b78a-36a2d7c002f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, Box([-1. -1. -8.], [1. 1. 8.], (3,), float32))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_shape = env.observation_space.shape[0]\n",
    "state_shape,env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1160910a-4535-451e-b3b9-10533e91a112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_shape = env.action_space.shape[0]\n",
    "action_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "677dd227-4ef8-4e39-bbdb-2076765d1706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-2.], dtype=float32), array([2.], dtype=float32)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_bound = [env.action_space.low,env.action_space.high]\n",
    "action_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edfb082e-6958-44a2-bb70-24fc00e46d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "tau = 0.001\n",
    "replay_buffer = 10000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b1e5578-f06b-4eaf-9ae0-1bf1438d6d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0365a536-a28b-45af-8ce9-046be56b443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DDPG import ddpg_keras as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05d406c9-09cc-4532-bd62-df19121b7944",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2\n",
    "ou_noise = dd.OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1903358d-2554-439d-9e43-4c807fe9f6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 23:59:24.744818: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: UNKNOWN ERROR (100)\n",
      "2022-02-01 23:59:24.744875: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (MSI): /proc/driver/nvidia/version does not exist\n",
      "2022-02-01 23:59:24.745208: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "ddpg = dd.DDPG(action_shape,state_shape,lower_bound,upper_bound,gamma,tau,buffer_max_len=5000,batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b63b4cef-9727-4671-b908-5d819997de41",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 50\n",
    "num_timesteps = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51ea95b1-98d7-4ab6-bbb0-a23dca8cd047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: average reward: -9.085268032887575 ,total reward:-4542.634016443787\n",
      "Episode 1: average reward: -8.820533003610654 ,total reward:-4410.266501805327\n",
      "Episode 2: average reward: -8.898488669046786 ,total reward:-4449.244334523393\n",
      "Episode 3: average reward: -9.068600458909977 ,total reward:-4534.300229454989\n",
      "Episode 4: average reward: -9.274167775856009 ,total reward:-4637.0838879280045\n",
      "Episode 5: average reward: -9.171068598974857 ,total reward:-4585.534299487429\n",
      "Episode 6: average reward: -8.898475574312807 ,total reward:-4449.237787156403\n",
      "Episode 7: average reward: -8.80753511410177 ,total reward:-4403.7675570508845\n",
      "Episode 8: average reward: -9.302035395799873 ,total reward:-4651.017697899936\n",
      "Episode 9: average reward: -8.90111100507822 ,total reward:-4450.55550253911\n",
      "Episode 10: average reward: -8.512629303810064 ,total reward:-4256.3146519050315\n",
      "Episode 11: average reward: -8.673982460152741 ,total reward:-4336.991230076371\n",
      "Episode 12: average reward: -8.558627170854272 ,total reward:-4279.313585427136\n",
      "Episode 13: average reward: -9.253979139164235 ,total reward:-4626.989569582118\n",
      "Episode 14: average reward: -8.67506338105118 ,total reward:-4337.5316905255895\n",
      "Episode 15: average reward: -8.995462542677181 ,total reward:-4497.731271338591\n",
      "Episode 16: average reward: -8.34503351382932 ,total reward:-4172.51675691466\n",
      "Episode 17: average reward: -8.678188294574413 ,total reward:-4339.094147287206\n",
      "Episode 18: average reward: -9.212948902057697 ,total reward:-4606.474451028848\n",
      "Episode 19: average reward: -8.98607687003989 ,total reward:-4493.038435019945\n",
      "Episode 20: average reward: -4.94375872856779 ,total reward:-2471.879364283895\n",
      "Episode 21: average reward: -8.79614754547492 ,total reward:-4398.07377273746\n",
      "Episode 22: average reward: -5.576032485341232 ,total reward:-2788.016242670616\n",
      "Episode 23: average reward: -8.995355420477772 ,total reward:-4497.677710238886\n",
      "Episode 24: average reward: -8.73734409147514 ,total reward:-4368.67204573757\n",
      "Episode 25: average reward: -8.95177335682644 ,total reward:-4475.88667841322\n",
      "Episode 26: average reward: -9.225087017770878 ,total reward:-4612.543508885439\n",
      "Episode 27: average reward: -9.105087795861449 ,total reward:-4552.543897930725\n",
      "Episode 28: average reward: -9.137006761160256 ,total reward:-4568.503380580128\n",
      "Episode 29: average reward: -9.115009565189368 ,total reward:-4557.504782594684\n",
      "Episode 30: average reward: -8.837073481002037 ,total reward:-4418.536740501018\n",
      "Episode 31: average reward: -8.769485704229794 ,total reward:-4384.742852114897\n",
      "Episode 32: average reward: -9.172192025891345 ,total reward:-4586.096012945673\n",
      "Episode 33: average reward: -9.276479189898803 ,total reward:-4638.239594949401\n",
      "Episode 34: average reward: -9.041263360674714 ,total reward:-4520.631680337357\n",
      "Episode 35: average reward: -9.266841520563025 ,total reward:-4633.420760281512\n",
      "Episode 36: average reward: -9.081975657082264 ,total reward:-4540.987828541132\n",
      "Episode 37: average reward: -8.753835739520671 ,total reward:-4376.917869760336\n",
      "Episode 38: average reward: -9.139806684672111 ,total reward:-4569.903342336055\n",
      "Episode 39: average reward: -9.019841806509374 ,total reward:-4509.920903254687\n",
      "Episode 40: average reward: -4.663079584783985 ,total reward:-2331.5397923919927\n",
      "Episode 41: average reward: -9.391658273747968 ,total reward:-4695.829136873984\n",
      "Episode 42: average reward: -9.510075005402774 ,total reward:-4755.037502701387\n",
      "Episode 43: average reward: -8.936380076683553 ,total reward:-4468.190038341777\n",
      "Episode 44: average reward: -9.28924506029932 ,total reward:-4644.622530149661\n",
      "Episode 45: average reward: -9.147322364131169 ,total reward:-4573.661182065584\n",
      "Episode 46: average reward: -9.489274843762177 ,total reward:-4744.637421881088\n",
      "Episode 47: average reward: -8.91469052578483 ,total reward:-4457.345262892414\n",
      "Episode 48: average reward: -8.554177678866623 ,total reward:-4277.0888394333115\n",
      "Episode 49: average reward: -8.365670140028952 ,total reward:-4182.835070014476\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.squeeze(state)\n",
    "    state = state[np.newaxis,:]\n",
    "    minibatch = []\n",
    "    episode_avg_reward = 0\n",
    "    for j in range(num_timesteps):\n",
    "        action = ddpg.policy(state,ou_noise)\n",
    "        next_state,reward,done,_ = env.step(action)\n",
    "        episode_avg_reward += reward\n",
    "        next_state = np.expand_dims(np.squeeze(next_state),0)\n",
    "        #print(next_state)\n",
    "        ddpg.replay_buffer(state,action,reward,next_state,done)\n",
    "        state = next_state\n",
    "    ddpg.train(batch_size=batch_size)\n",
    "    print(f\"Episode {i}: average reward: {episode_avg_reward/num_timesteps} ,total reward:{episode_avg_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
